declare enum ChatMessageRole {
    User = "user",
    Assistant = "assistant"
}

declare enum ModelType {
    ChatGLMPro = "chatglm_pro",
    ChatGLMStd = "chatglm_std",
    ChatGLMLite = "chatglm_lite",
    ChatGLMTurbo = "chatglm_turbo",
    CharacterGLM = "characterglm",
    TextEmbedding = "text_embedding"
}

declare enum TaskStatus {
    Processing = "PROCESSING",
    Success = "SUCCESS",
    Fail = "FAIL"
}

interface Response<T> {
    code: number;
    msg: string;
    success: boolean;
    data: T;
}

type AsyncInvokeResponse = Response<{
    request_id: string;
    task_id: string;
    task_status: TaskStatus;
}>;

interface ChatMessage {
    role: ChatMessageRole;
    content: string;
}

interface Usage {
    completion_tokens: number;
    prompt_tokens: number;
    total_tokens: string;
}

type CreateEmbeddingResponse = Response<{
    request_id: string;
    task_id: string;
    task_status: TaskStatus;
    embedding: number[];
    usage: Usage;
}>;

type InvokeResponse = Response<{
    request_id: string;
    task_id: string;
    task_status: TaskStatus;
    choices: ChatMessage[];
    usage: Usage;
}>;

interface CommonRequestOptions {
    requestId?: string;
    incremental?: boolean;
    token?: string;
    timeout?: number;
}
interface ChatModelRequestOptions extends CommonRequestOptions {
    model: ModelType.ChatGLMLite | ModelType.ChatGLMStd | ModelType.ChatGLMPro | ModelType.ChatGLMTurbo;
    messages: ChatMessage[];
    temperature?: number;
    topP?: number;
}
interface CharacterModelRequestOptions extends CommonRequestOptions {
    model: ModelType.CharacterGLM;
    messages: ChatMessage[];
    meta: {
        userInfo: string;
        userName?: string;
        botInfo: string;
        botName: string;
    };
}
interface EmbeddingModelRequestOptions extends CommonRequestOptions {
    model: ModelType.TextEmbedding;
    input: string;
}
type RequestOptions = ChatModelRequestOptions | CharacterModelRequestOptions | EmbeddingModelRequestOptions;

interface ZhipuSSEResponse {
    id: string;
    event: 'add' | 'finish';
    data: string;
    meta: string;
}
type SSEResponse = {
    id: string;
    event: 'add';
    data: string;
} | {
    id: string;
    event: 'finish';
    data: string;
    meta: {
        task_status: TaskStatus;
        usage: Usage;
        task_id: string;
        request_id: string;
    };
};

interface ZhipuAIOptions {
    apiKey: string;
    apiPrefix: string;
    browser: boolean;
    tokenTTL: number;
    tokenRefreshTTL: number;
}

declare function generateToken(apiKey: string, timestamp: number, ttl: number): never;

declare class ZhipuAI {
    private cachedToken;
    private options;
    constructor(opts?: Partial<ZhipuAIOptions>);
    private getToken;
    private buildApiUrl;
    private buildRequestBody;
    private buildAxiosRequestConfig;
    private handleError;
    private request;
    invoke(options: ChatModelRequestOptions | CharacterModelRequestOptions): Promise<InvokeResponse['data']>;
    invoke(options: EmbeddingModelRequestOptions): Promise<CreateEmbeddingResponse['data']>;
    asyncInvoke(options: ChatModelRequestOptions | CharacterModelRequestOptions): Promise<AsyncInvokeResponse['data']>;
    queryAsyncInvokeResult(taskId: string, options?: Pick<RequestOptions, 'timeout' | 'token'>): Promise<InvokeResponse['data']>;
    sseInvoke(options: ChatModelRequestOptions | CharacterModelRequestOptions): Promise<AsyncGenerator<SSEResponse, void, unknown>>;
}

export { type AsyncInvokeResponse, type CharacterModelRequestOptions, type ChatMessage, ChatMessageRole, type ChatModelRequestOptions, type CommonRequestOptions, type CreateEmbeddingResponse, type EmbeddingModelRequestOptions, type InvokeResponse, ModelType, type RequestOptions, type Response, type SSEResponse, TaskStatus, type Usage, ZhipuAI, type ZhipuAIOptions, type ZhipuSSEResponse, generateToken };
